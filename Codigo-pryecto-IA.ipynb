%tensorflow_version 2.x

import tensorflow as tf

import os
import time

from matplotlib import pyplot as plt
from IPython import display

import numpy as np

PATH = "/content/drive/MyDrive/flowersData/"
INPATH = PATH + 'inputFlowers'
OUTPATH = PATH + '/targetFlowers'
CKPATH = PATH + '/checkpoints'

imgurls = !ls -1 "{INPATH}"

#Numero de imagenes
n = 5
train_n = round(n * 0.80)

randurls = np.copy(imgurls)
np.random.seed(23)
np.random.shuffle(randurls)

tr_urls = randurls[:train_n]
ts_urls = randurls[train_n:n]
#Imprime el numero de img en dataset general ,dataset de 'entrenamiento'  y dataset 'test'
print(len(imgurls), len(tr_urls), len(ts_urls))
imgurls

from google.colab import drive
drive.mount('/content/drive')


#Normalizar el tamaño de las imagenes
IMG_WIDTH = 256
IMG_HEIGHT = 256

#rescalar imagenes (imgEntrada, imgSalida, altura, largo )
def resize(inimg, tgimg, height, width):
  inimg = tf.image.resize(inimg, [height, width])
  tgimg = tf.image.resize(tgimg, [height, width])

  return inimg, tgimg

#normalizar imgagenes al rango [-1,1]
def normalize(inimg, tgimg):
  inimg = (inimg / 127.5) - 1
  tgimg = (tgimg / 127.5) - 1
  
  return inimg, tgimg

@tf.function()
#Aumentacion de datos
#Generar n nuevas imagnes de otros tamaños
def random_jitter(inimg, tgimg):
 inimg, tgimg = resize(inimg, tgimg, 286,286)
 stacked_image = tf.stack([inimg, tgimg], axis=0)
 cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])

 inimg, tgimg = cropped_image[0], cropped_image[1]

#Crear imagenes para distintos angulos de imagenes
 if tf.random.uniform(()) > 0.5:
   inimg = tf.image.flip_left_right(inimg)
   tgimg = tf.image.flip_left_right(tgimg)

 return inimg, tgimg

#Se crean las imagenes
def load_image(filename, augment=True):
  inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(INPATH + '/' + filename)), tf.float32)[..., :3]
  tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(OUTPATH + '/' + filename)), tf.float32)[..., :3]

  inimg, tgimg = resize(inimg, tgimg, IMG_HEIGHT, IMG_WIDTH)
  
  if augment:
    inimg, tgimg = random_jitter(inimg, tgimg) 
  inimg, tgimg = normalize(inimg, tgimg)
  
  return inimg, tgimg

def load_train_image(filename):
  return load_image(filename, True)

def load_test_image(filename):
  return load_image(filename, False)

#Funcion para visualizar las imagenes
plt.imshow(((load_train_image(randurls[0])[1]) + 1) / 2)


#Implementar la Api de Dataset de Tensorflow
# Genera un dataset a partir de elementos que nosotros le demos
# se le pasa el listado de las urls anteriormente creadas
train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)
#Hace el mapeo (carga las imagenes)
train_dataset = train_dataset.map(load_train_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
#Aplicar lotes para el procesamiento
train_dataset = train_dataset.batch(1)

test_dataset = tf.data.Dataset.from_tensor_slices(ts_urls)
test_dataset = test_dataset.map(load_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_dataset = test_dataset.batch(1)

# Todo lo anterior es el proceso de carga de datos


#Diseño de Pix2Pix
from tensorflow.keras import *
from tensorflow.keras.layers import *

#compresion de imagen (pasa por capas para comprimir la imagen) encoder
def downsample(filters, apply_batchnorm=True):
  
  #Define la secuencia de capas
  result = Sequential()
  
  initializer = tf.random_normal_initializer(0, 0.02)

  #capa convolucional
  result.add(Conv2D(filters,
                    kernel_size=4,
                    strides=2,
                    padding="same",
                    kernel_initializer=initializer,
                    use_bias=not apply_batchnorm))
  if apply_batchnorm:
    #capa batchnorm
    result.add(BatchNormalization())

    #capa de activacion
  result.add(LeakyReLU())

  return result
# Nos crea un minibloque para hacer pasar la imagen por las capas definidas.
downsample(64)


#Recupera la imagen hecha en el encoder anterior
def upsample(filters, apply_dropout=False):

  result = Sequential()

  initializer = tf.random_normal_initializer(0, 0.02)

  #capa convolucional
  result.add(Conv2DTranspose(filters,
                             kernel_size=4,
                             strides=2,
                             padding="same",
                             kernel_initializer=initializer,
                             use_bias=False))
  
    #capa batchnorm
  result.add(BatchNormalization())

  if apply_dropout:
    result.add(Dropout(0.5))
    #capa de activacion
  result.add(ReLU())

  return result

upsample(64)


def Generator():
  #Especificar una capa de entrada de datos
  inputs = tf.keras.layers.Input(shape=[None, None,3])
  
  #Dimenciones de las imagenes
  down_stack = [
               downsample(64, apply_batchnorm=False),
               downsample(128),
               downsample(256),
               downsample(512),
               downsample(512),
               downsample(512),
               downsample(512),
               downsample(512),
                ]
  up_stack = [
             upsample(512, apply_dropout=True),
             upsample(512, apply_dropout=True),
             upsample(512, apply_dropout=True),
             upsample(512),
             upsample(256),
             upsample(128),
             upsample(64),
            ]
  #Capa para buscar la imagen (generar img)
  initializer = tf.random_normal_initializer(0, 0.02)
  last = Conv2DTranspose(filters = 3,
                        kernel_size = 4,
                        strides = 2,
                        padding = "same",
                        kernel_initializer = initializer,
                        activation = "tanh")
  
  x = inputs
  s = []
 
  concat = Concatenate()
 #Recorrer las capas del decoder
  for down in down_stack:
   x = down(x)
   s.append(x)
  s = reversed(s[:-1])
 
 #Hacer el decodificador
  for up, sk in zip(up_stack, s):
   x = up(x)
   x = concat([x,sk])
  last = last(x)

#def random_jiter(inimg, tgimg):
  #inimg, tgimg = resize(inimg, tgimg, 286,286)
  return Model(inputs = inputs, outputs = last)
# Instanciar el generador
generator = Generator()
# Tomar una imagen y probar el generador


def Discriminator():
  ini = Input(shape=[None,None, 3], name="input_img")
  gen = Input(shape=[None,None, 3], name="gener_img")
  #Cancatenar las imagenes para generar un tensor
  con = concatenate([ini, gen])

  #Definir los flitros a aplicar en los canales
  initializer = tf.random_normal_initializer(0, 0.02)

  down1 = downsample(64, apply_batchnorm=False)(con)
  down2 = downsample(128)(down1)
  down3 = downsample(256)(down2)
  down4 = downsample(512)(down3)

  last = tf.keras.layers.Conv2D(filters = 1,
                                kernel_size = 4,
                                strides = 1,
                                kernel_initializer = initializer,
                                padding = "same")(down4)
  return tf.keras.Model(inputs=[ini, gen], outputs=last)

discriminator = Discriminator()


#Calular los pixeles de la imagenes que se van obteniendo
loss_object = tf.keras.losses.BinaryCrossentropy(from_logits = True)


#Evaluar el discriminador
def discriminator_loss(disc_real_output, disc_generated_output):
  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)
  #Detecta si las imagenes que se van obteniendo
  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)

  total_disc_loss = real_loss + generated_loss

  return total_disc_loss


  LAMBDA = 100
#Generar imagen realista
def generator_loss(disc_generated_output, gen_output, target):
  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)

  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))

  total_gen_loss = gan_loss + (LAMBDA * l1_loss)

  return total_gen_loss


  import os
#Defenir los optimizadores
generator_optimizer     = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)

checkpoint_prefix = os.path.join(CKPATH, "ckpt")
#Guardar estados de las ejecuciones de los optimixadores de los generadores y discriminador
checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,
                                 discriminator_optimizer = discriminator_optimizer,
                                 generator = generator,
                                 discriminator = discriminator)
#checkpoint.restore(tf.train.latest_checkpoint(CKPATH)).assert_consumed()


#Evaluar el comportamiento del generador pasandoles imagenes
def generate_images(model, test_input, tar, save_filename=False, display_imgs=True):

  prediction = model(test_input, training=True)

  if save_filename:
    tf.keras.preprocessing.image.save_img(PATH + '/output/' + save_filename + '.jpg', prediction[0,...])

  plt.figure(figsize=(10,10))

  display_list = [test_input[0], tar[0], prediction[0]]
  title = ['Input Image', 'Ground Truth', 'Predicted Image']

  if display_imgs:
    for i in range(3):
      plt.subplot(1, 3, i + 1)
      plt.title(title[i])
      plt.imshow(display_list[i] * 0.5 + 0.5)
      plt.axis('off')

  plt.show()



  @tf.function()
def train_step(input_image, target):
  with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:
    output_image = generator(input_image, training=True)
    output_gen_discr = discriminator([output_image, input_image], training=True)
    output_trg_discr = discriminator([target, input_image], training=True)
    discr_loss = discriminator_loss(output_trg_discr, output_gen_discr)
    gen_loss = generator_loss(output_gen_discr, output_image, target)

    generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)
    discriminator_grads = discr_tape.gradient(discr_loss, discriminator.trainable_variables)
    generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))



    from IPython.display import clear_output
#Define las rutinas del entrenamiento
def train(dataset, epochs):
  for epoch in range(epochs):

    imgi = 0
    for input_image, target in dataset:
      print('epoch ' + str(epoch) + ' - train: ' + str(imgi) + '/' + str(len(tr_urls)))
      imgi += 1
      train_step(input_image, target)

      clear_output(wait=True)

    imgi=0
    for inp, tar in test_dataset.take(5):
      generate_images(generator, inp, tar, str(imgi) + '_' + str(epoch), display_imgs=True)
      imgi+=1
    if (epoch + 1) % 50 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)


      train(train_dataset, 100)